import tensorflow as tf
import os
import pandas as pd
import numpy as np
import csv


CSV_dir = "CSVFiles"

files = [f for f in os.listdir(CSV_dir) if os.path.isfile(os.path.join(CSV_dir, f))]

data_arr = []

for i in range(len(files)):
    file_name = f"interval{i+1}.csv"
    file_path = os.path.join(CSV_dir, file_name)
    with open(file_path,  newline='') as f:
        reader = csv.reader(f)
        data = [row for row in reader]
        if i > 0 :
            data = data[1:]
        data_arr = data_arr + data
    
    

df = pd.DataFrame(data_arr)
parame =df.iloc[0]
df.columns = df.iloc[0]
df = df[1:]
for i in range(len(parame)):
    if i >0:
        df[f'{parame[i]}'] = pd.to_numeric(df[f'{parame[i]}'], errors='coerce')

df.iloc[0]



df.index = pd.to_datetime(df['Timestamp'],  format = '%Y-%m-%d %H:%M:%S')
df[:25]



Booster_Current = df['BO-DI-DCCT1_getDcctCurrent']
Booster_Current.plot()


df.isna().mean()


corr_matrix = df.corr(numeric_only=True)
corr_matrix['BO-DI-DCCT1_getDcctCurrent']


import os, random
import numpy as np, pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
import torch, torch.nn as nn
from torch.utils.data import Dataset, DataLoader

DATA_DIR = "cut_intervals/cropped_files" 
EXPECTED_TIMESTEPS = 200
BATCH_SIZE = 8
LR = 1e-3
EPOCHS = 200
SEED = 42

random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")


csv_files = [os.path.join(DATA_DIR, f) for f in os.listdir(DATA_DIR) if f.endswith(".csv")]
X_list, y_list = [], []
for file in csv_files:
    df = pd.read_csv(file)
    if df.shape[0] != EXPECTED_TIMESTEPS: continue
    if "BO-DI-DCCT1_getDcctCurrent" not in df.columns: continue
    feats = df.drop(columns=["BO-DI-DCCT1_getDcctCurrent", "Timestamp"]).values  # (200,25)
    tgt = df["BO-DI-DCCT1_getDcctCurrent"].iloc[-1]
    X_list.append(feats); y_list.append(tgt)

X = np.array(X_list); y = np.array(y_list)
print("Loaded:", X.shape, y.shape)
if X.shape[0]==0: raise RuntimeError("No data loaded")


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)
print("Train/test:", X_train.shape[0], X_test.shape[0])


Ntrain, T, F = X_train.shape
scaler_X = StandardScaler().fit(X_train.reshape(-1, F))
scaler_y = StandardScaler().fit(y_train.reshape(-1,1))

def scale_X(X_in):
    n,t,f = X_in.shape
    flat = X_in.reshape(-1,f)
    flat_s = scaler_X.transform(flat)
    return flat_s.reshape(n,t,f)

X_train_s = scale_X(X_train)
X_test_s = scale_X(X_test)
y_train_s = scaler_y.transform(y_train.reshape(-1,1)).ravel()


class SeqDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.float32)
    def __len__(self): return len(self.X)
    def __getitem__(self, idx): return self.X[idx], self.y[idx]

train_loader = DataLoader(SeqDataset(X_train_s, y_train_s), batch_size=BATCH_SIZE, shuffle=True)


class LSTMRegressor(nn.Module):
    def __init__(self, input_size, hidden_size=128, num_layers=2, dropout=0.2):
        super().__init__()
        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,
                            num_layers=num_layers, batch_first=True, dropout=dropout)
        self.fc = nn.Sequential(nn.Linear(hidden_size, 64), nn.ReLU(), nn.Dropout(0.2), nn.Linear(64,1))
    def forward(self, x):

        out, (hn, cn) = self.lstm(x)            
        last = out[:, -1, :]                    
        return self.fc(last).squeeze(-1)

model = LSTMRegressor(input_size=F).to(DEVICE)
opt = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-5)
loss_fn = nn.MSELoss()


model.train()
for epoch in range(1, EPOCHS+1):
    losses = []
    for xb, yb in train_loader:
        xb = xb.to(DEVICE); yb = yb.to(DEVICE)
        opt.zero_grad()
        out = model(xb)
        loss = loss_fn(out, yb)
        loss.backward(); opt.step()
        losses.append(loss.item())
    if epoch % 10 == 0 or epoch==1:
        print(f"Epoch {epoch}/{EPOCHS}, Train Loss: {np.mean(losses):.4f}")


model.eval()
with torch.no_grad():
    Xtest_t = torch.tensor(X_test_s, dtype=torch.float32).to(DEVICE)
    preds_s = model(Xtest_t).cpu().numpy().ravel()
    preds = scaler_y.inverse_transform(preds_s.reshape(-1,1)).ravel()

mse = mean_squared_error(y_test, preds)
r2 = r2_score(y_test, preds)
print(f"\nLSTM Test MSE: {mse:.4f}, RÂ²: {r2:.4f}")



def df_to_X_y(df, window_size=5):
    df_as_np = df.drop(columns=["Timestamp"]).to_numpy()

    X =[]
    y =[]

    for i in range(len(df_as_np)-window_size):
        row = [[a] for a in df_as_np[i:i+5]]
        X.append(row)
        label = df_as_np[i+5][:25]
        y.append(label)
    return np.array(X), np.array(y)


X, y =df_to_X_y(df)
X.shape , y.shape


X_train , y_train = X[:80000], y[:80000]
X_val , y_val = X[80000:90000], y[80000:90000]
X_test , y_test = X[90000:], y[90000:]

X_train.shape, X_val.shape, X_test.shape,y_train.shape, y_val.shape, y_test.shape


from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import *
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.losses import MeanAbsoluteError
from tensorflow.keras.metrics import RootMeanSquaredError
from tensorflow.keras.optimizers import Adam


model1 = Sequential()
model1.add(InputLayer((5, 26)))
model1.add(LSTM(64))
model1.add(Dense(8, 'relu'))
model1.add(Dense(1, 'linear'))
model1.summary




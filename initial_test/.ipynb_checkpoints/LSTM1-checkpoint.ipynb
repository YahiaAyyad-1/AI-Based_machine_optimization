{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPtuFTKUIjn3vfCF1PQloQv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-BPvd06F36Iq","executionInfo":{"status":"ok","timestamp":1760196488601,"user_tz":-180,"elapsed":178262,"user":{"displayName":"Mohammed Ahmed","userId":"08072415299754705346"}},"outputId":"22171ea9-1948-4545-bfe1-ac3e4cb338b7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Device: cpu\n","Loaded 107 intervals, shape X=(107, 200, 25), y=(107,)\n","Split sizes -> train, val, test: 68 17 22\n","Epoch 1 train_loss=0.306055 val_loss=0.179815\n","Epoch 10 train_loss=0.276545 val_loss=0.134025\n","Epoch 20 train_loss=0.229675 val_loss=0.130803\n","Epoch 30 train_loss=0.264877 val_loss=0.128937\n","Epoch 40 train_loss=0.166108 val_loss=0.054653\n","Epoch 50 train_loss=0.072013 val_loss=0.024627\n","Epoch 60 train_loss=0.118782 val_loss=0.221707\n","Epoch 70 train_loss=0.139208 val_loss=0.246821\n","Early stopping at epoch 73\n","\n","Final Test metrics: MSE= 0.16570247627949575 R2= 0.9408072765935664\n","Baseline (train mean) MSE= 2.8372411847213352\n","Saved model + scalers to lstm_artifacts\n"]}],"source":["#LSTM bnst5dm al data ale esmha cut intervals fixed BUT many parts are edited by chatgpt should be studied\n","import os, random\n","import numpy as np, pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import mean_squared_error, r2_score\n","import matplotlib.pyplot as plt\n","import joblib\n","import torch, torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","\n","# ---------------- CONFIG ----------------\n","DATA_DIR = \"/content/Cut_fixed_data\"   # <- set your data folder\n","EXPECTED_TIMESTEPS = 200\n","BATCH_SIZE = 8\n","LR = 1e-4\n","EPOCHS = 300\n","SEED = 42\n","PATIENCE = 25\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","OUT_DIR = \"lstm_artifacts\"\n","os.makedirs(OUT_DIR, exist_ok=True)\n","\n","# reproducibility\n","random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n","print(\"Device:\", DEVICE)\n","\n","# ---------------- LOAD DATA ----------------\n","def load_intervals(data_dir, expected_timesteps):\n","    files = sorted([os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith(\".csv\")])\n","    X_list, y_list, names = [], [], []\n","    for p in files:\n","        df = pd.read_csv(p)\n","        if df.shape[0] != expected_timesteps:\n","            continue\n","        if \"BO-DI-DCCT1_getDcctCurrent\" not in df.columns:\n","            continue\n","        feats = df.drop(columns=[\"BO-DI-DCCT1_getDcctCurrent\", \"Timestamp\"], errors='ignore').values\n","        tgt = df[\"BO-DI-DCCT1_getDcctCurrent\"].iloc[-1]\n","        X_list.append(feats); y_list.append(tgt); names.append(os.path.basename(p))\n","    X = np.array(X_list); y = np.array(y_list)\n","    return X, y, names\n","\n","X, y, names = load_intervals(DATA_DIR, EXPECTED_TIMESTEPS)\n","if len(X) == 0:\n","    raise RuntimeError(\"No data loaded. Check DATA_DIR and file format.\")\n","print(f\"Loaded {len(X)} intervals, shape X={X.shape}, y={y.shape}\")\n","\n","# ---------------- OUTLIER-aware stratified split ----------------\n","# bin the target into quantiles to keep distribution similar in splits\n","quantiles = np.quantile(y, np.linspace(0,1,6))\n","y_bins = np.digitize(y, quantiles[1:-1])  # categories 0..4\n","try:\n","    X_trval, X_test, y_trval, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED, stratify=y_bins)\n","except:\n","    X_trval, X_test, y_trval, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n","\n","X_train, X_val, y_train, y_val = train_test_split(X_trval, y_trval, test_size=0.2, random_state=SEED)\n","print(\"Split sizes -> train, val, test:\", X_train.shape[0], X_val.shape[0], X_test.shape[0])\n","\n","# ---------------- SCALERS (fit on TRAIN only) ----------------\n","Ntrain, T, F = X_train.shape\n","scaler_X = StandardScaler().fit(X_train.reshape(-1, F))\n","scaler_y = StandardScaler().fit(y_train.reshape(-1,1))\n","\n","def scale_X(X_in):\n","    n,t,f = X_in.shape\n","    flat = X_in.reshape(-1,f)\n","    flat_s = scaler_X.transform(flat)\n","    return flat_s.reshape(n,t,f)\n","\n","X_train_s = scale_X(X_train)\n","X_val_s   = scale_X(X_val)\n","X_test_s  = scale_X(X_test)\n","y_train_s = scaler_y.transform(y_train.reshape(-1,1)).ravel()\n","y_val_s   = scaler_y.transform(y_val.reshape(-1,1)).ravel()\n","\n","# ---------------- DATASET / DATALOADER ----------------\n","class SeqDataset(Dataset):\n","    def __init__(self, X, y):\n","        self.X = torch.tensor(X, dtype=torch.float32)\n","        self.y = torch.tensor(y, dtype=torch.float32)\n","    def __len__(self): return len(self.X)\n","    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n","\n","train_loader = DataLoader(SeqDataset(X_train_s, y_train_s), batch_size=BATCH_SIZE, shuffle=True)\n","val_loader   = DataLoader(SeqDataset(X_val_s, y_val_s), batch_size=BATCH_SIZE, shuffle=False)\n","test_tensor  = torch.tensor(X_test_s, dtype=torch.float32).to(DEVICE)\n","\n","# ---------------- MODEL ----------------\n","class LSTMRegressor(nn.Module):\n","    def __init__(self, input_size, hidden_size=128, num_layers=2, dropout=0.2):\n","        super().__init__()\n","        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n","                            num_layers=num_layers, batch_first=True, dropout=dropout)\n","        self.fc = nn.Sequential(nn.Linear(hidden_size,64), nn.ReLU(), nn.Dropout(0.2), nn.Linear(64,1))\n","    def forward(self, x):\n","        out, _ = self.lstm(x)            # out: (batch, T, hidden)\n","        last = out[:, -1, :]             # take final timestep\n","        return self.fc(last).squeeze(-1)\n","\n","model = LSTMRegressor(input_size=F).to(DEVICE)\n","opt = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-6)\n","loss_fn = nn.SmoothL1Loss()   # Huber-like (robust)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, factor=0.5, patience=10)\n","\n","# ---------------- TRAIN with early stopping ----------------\n","best_val = float('inf'); patience = 0\n","for epoch in range(1, EPOCHS+1):\n","    model.train()\n","    train_losses = []\n","    for xb, yb in train_loader:\n","        xb = xb.to(DEVICE); yb = yb.to(DEVICE)\n","        opt.zero_grad()\n","        out = model(xb)\n","        loss = loss_fn(out, yb)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)\n","        opt.step()\n","        train_losses.append(loss.item())\n","    model.eval()\n","    val_losses = []\n","    with torch.no_grad():\n","        for xb, yb in val_loader:\n","            xb = xb.to(DEVICE); yb = yb.to(DEVICE)\n","            val_losses.append(float(loss_fn(model(xb), yb).item()))\n","    avg_train = float(np.mean(train_losses)); avg_val = float(np.mean(val_losses))\n","    scheduler.step(avg_val)\n","    if epoch % 10 == 0 or epoch==1:\n","        print(f\"Epoch {epoch} train_loss={avg_train:.6f} val_loss={avg_val:.6f}\")\n","    if avg_val + 1e-8 < best_val:\n","        best_val = avg_val; patience = 0\n","        torch.save(model.state_dict(), os.path.join(OUT_DIR, \"best_lstm.pth\"))\n","    else:\n","        patience += 1\n","        if patience >= PATIENCE:\n","            print(\"Early stopping at epoch\", epoch)\n","            break\n","\n","# ---------------- EVAL on test ----------------\n","model.load_state_dict(torch.load(os.path.join(OUT_DIR, \"best_lstm.pth\"), map_location=DEVICE))\n","model.eval()\n","with torch.no_grad():\n","    preds_s = model(test_tensor).cpu().numpy().ravel()\n","preds = scaler_y.inverse_transform(preds_s.reshape(-1,1)).ravel()\n","\n","mse = mean_squared_error(y_test, preds); r2 = r2_score(y_test, preds)\n","print(\"\\nFinal Test metrics: MSE=\", mse, \"R2=\", r2)\n","print(\"Baseline (train mean) MSE=\", mean_squared_error(y_test, np.full_like(y_test, np.mean(y_train))))\n","\n","# ---------------- SAVE artifacts ----------------\n","joblib.dump(scaler_X, os.path.join(OUT_DIR, \"scaler_X.pkl\"))\n","joblib.dump(scaler_y, os.path.join(OUT_DIR, \"scaler_y.pkl\"))\n","torch.save(model.state_dict(), os.path.join(OUT_DIR, \"lstm_final.pth\"))\n","print(\"Saved model + scalers to\", OUT_DIR)\n","\n"]},{"cell_type":"code","source":[],"metadata":{"id":"_Wzz_0pq4DQs"},"execution_count":null,"outputs":[]}]}